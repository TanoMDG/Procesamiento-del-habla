{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1odMkSDgl9iapHtBW5zJ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups**\n","\n","## -  **Consignas:**\n","\n","## **1**. **Vectorizar documentos**.\n"," Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n","Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n","la similaridad según el contenido del texto y la etiqueta de clasificación.\n","\n","\n","## **2**. **Entrenar modelos de clasificación Naïve Bayes**\n","para maximizar el desempeño de clasificación\n","(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámetros\n","de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n","y ComplementNB.\n","\n","## **3**. **Transponer la matriz documento-término**.\n","De esa manera se obtiene una matriz\n","término-documento que puede ser interpretada como una colección de vectorización de palabras.\n","Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."],"metadata":{"id":"xTVHdXD3aTVj"}},{"cell_type":"code","source":["# Importo las librerías necesarias\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.datasets import fetch_20newsgroups\n","import numpy as np\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB\n","from sklearn.metrics import f1_score\n","\n","# Se utiliza la función \"fetch_20newsgroups\" para cargar el subconjunto de entrenamiento (train) del dataset \"20 newsgroups\",\n","# removiendo encabezados, pies de página y citas de los correos electrónicos\n","newsgroups_train = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n","\n","# Instanciar el vectorizador TF-IDF\n","# Este vectorizador transformará los textos en una matriz esparsa en la que cada fila representará un documento, y cada columna\n","# representará un término (palabra o token) con su valor de TF-IDF.\n","tfidfvect = TfidfVectorizer()\n","\n","# Accesso al texto\n","newsgroups_train.data[0]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"dDLQnryFJSCU","executionInfo":{"status":"ok","timestamp":1728360344016,"user_tz":180,"elapsed":1771,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"04509f40-9190-41c9-c716-7a1e77cffdda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Fit-transformar los datos de entrenamiento, ajusta el vectorizador (fit) para aprender el vocabulario de los documentos y luego transforma los textos en una matriz TF-IDF\n","X_train = tfidfvect.fit_transform(newsgroups_train.data)\n","\n","# Obtener el vocabulario\n","vocabulary = tfidfvect.vocabulary_\n","\n","# Mostrar algunas palabras del vocabulario\n","words = list(vocabulary.keys()) # Convierte el conjunto de claves del diccionario vocabulary en una lista de palabras\n","print(f\"Cantidad total de palabras en el vocabulario: {len(words)}\") # Imprime el número total de palabras en el vocabulario\n","print(\"Primeras palabras en el vocabulario:\", words[:20])  # Muestra las primeras 20 palabras\n","print(f\"shape: {X_train.shape}\")\n","print(f\"cantidad de documentos: {X_train.shape[0]}\")\n","print(f\"cantidad de términos: {X_train.shape[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKSJ1FNwTc2N","executionInfo":{"status":"ok","timestamp":1728360435717,"user_tz":180,"elapsed":2293,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"2585a934-62ee-4760-8335-c70b104cdb00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad total de palabras en el vocabulario: 101631\n","Primeras palabras en el vocabulario: ['was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'door', 'sports', 'looked']\n","shape: (11314, 101631)\n","cantidad de documentos: 11314\n","cantidad de términos: 101631\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728360663131,"user_tz":180,"elapsed":319,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"a4a00a44-89d1-4add-b20d-b7fec6eadb09","id":"t32lUwqgUorW"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4,  8, 19,  4, 14,  6])"]},"metadata":{},"execution_count":7}],"source":["\n","# Se guarda el vector target que contiene las etiquetas de clasificación (categorías) de cada documento en el conjunto de entrenamiento.\n","y_train = newsgroups_train.target\n","y_train[:15]\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"je5kxvQMDLvf","outputId":"8a83454e-a4d0-441d-b0d3-306c0ee57b63","executionInfo":{"status":"ok","timestamp":1728360907849,"user_tz":180,"elapsed":306,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n","====================================================================\n"]},{"output_type":"execute_result","data":{"text/plain":["['alt.atheism',\n"," 'comp.graphics',\n"," 'comp.os.ms-windows.misc',\n"," 'comp.sys.ibm.pc.hardware',\n"," 'comp.sys.mac.hardware',\n"," 'comp.windows.x',\n"," 'misc.forsale',\n"," 'rec.autos',\n"," 'rec.motorcycles',\n"," 'rec.sport.baseball',\n"," 'rec.sport.hockey',\n"," 'sci.crypt',\n"," 'sci.electronics',\n"," 'sci.med',\n"," 'sci.space',\n"," 'soc.religion.christian',\n"," 'talk.politics.guns',\n"," 'talk.politics.mideast',\n"," 'talk.politics.misc',\n"," 'talk.religion.misc']"]},"metadata":{},"execution_count":13}],"source":["# Muestro las diferentes clases de cada grupo de noticias\n","print(f\"clases {np.unique(newsgroups_train.target)}\")\n","print(\"=\"*68)\n","newsgroups_train.target_names"]},{"cell_type":"code","source":["# Seleccionar aleatoriamente 5 documentos del conjunto de entrenamiento (sin reemplazo)\n","np.random.seed(6)  # Semilla para repetir la elección aleatoria\n","random_docs = np.random.choice(X_train.shape[0], 5, replace=False)\n","\n","# Iterar sobre cada índice seleccionado aleatoriamente en random_docs e imprimir la información\n","for idx in random_docs:\n","    print(f\"\\n\\033[1mDocumento original ID: {idx}\\033[0m\\n\")  # Texto en negrita para resaltar el documento\n","\n","    # Mostrar la categoría del documento original (Color azul)\n","    print(f\"Categoría del documento original: \\033[94m{newsgroups_train.target_names[y_train[idx]]}\\033[0m\\n\")\n","\n","    # Mostrar una parte del contenido del documento original (solo los primeros 120 caracteres)\n","    print(f\"Contenido (primeros 100 caracteres): {newsgroups_train.data[idx][:100]}...\\n\")\n","\n","    # Calcular la similaridad coseno entre el documento seleccionado y todos los demás\n","    cossim = cosine_similarity(X_train[idx], X_train)[0]\n","\n","    # Obtener los 5 documentos más similares (excluyendo el propio documento)\n","    mostsim = np.argsort(cossim)[::-1][1:6]\n","\n","    # Mostrar los documentos más similares, sus categorías y valores de similaridad\n","    print(f\"Documentos más similares:\\n\")\n","    for i, sim_idx in enumerate(mostsim, start=1):\n","        similarity_value = cossim[sim_idx]  # Valor de similaridad coseno\n","        print(f\"\\033[1mDocumento ID {sim_idx}:\\033[0m\")  # Negrita para el título\n","        print(f\"Categoría: \\033[92m{newsgroups_train.target_names[y_train[sim_idx]]}\\033[0m\")  # Categoría en color verde\n","        print(f\"Valor de similaridad: \\033[93m{similarity_value:.4f}\\033[0m\")  # Valor de similaridad en color amarillo\n","        print(f\"Contenido del documento (primeros 120 caracteres): {newsgroups_train.data[sim_idx][:120]}...\\n\")\n","\n","    # Separador visual para cada iteración\n","    print(\"<>\" * 70)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgoULs7EaxO3","executionInfo":{"status":"ok","timestamp":1728363187056,"user_tz":180,"elapsed":318,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"2856550b-ba75-433e-d902-360dddc0a6b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[1mDocumento original ID: 2097\u001b[0m\n","\n","Categoría del documento original: \u001b[94msci.med\u001b[0m\n","\n","Contenido (primeros 100 caracteres): \n","His _heart_? This jerk doesn't have a heart, and it beats me why you're\n","apologizing for him. In my ...\n","\n","Documentos más similares:\n","\n","\u001b[1mDocumento ID 8135:\u001b[0m\n","Categoría: \u001b[92msci.med\u001b[0m\n","Valor de similaridad: \u001b[93m0.2430\u001b[0m\n","Contenido del documento (primeros 120 caracteres): I need advice with a situation which occurred between me and a physican\n","which upset me.  I saw this doctor for a problem...\n","\n","\u001b[1mDocumento ID 9670:\u001b[0m\n","Categoría: \u001b[92msoc.religion.christian\u001b[0m\n","Valor de similaridad: \u001b[93m0.2313\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","[..]\n","\n","\n","Hello.  Firstly, what do you exactly mean by \"fundamentalist\"?  I will\n","for the time being assume that what you m...\n","\n","\u001b[1mDocumento ID 5829:\u001b[0m\n","Categoría: \u001b[92mtalk.politics.mideast\u001b[0m\n","Valor de similaridad: \u001b[93m0.2299\u001b[0m\n","Contenido del documento (primeros 120 caracteres): Accounts of Anti-Armenian Human Right Violations in Azerbaijan #013\n","                 Prelude to Current Events in Nagorn...\n","\n","\u001b[1mDocumento ID 3576:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.2133\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","Sorry, Nelson, but you forgot to ask me. If you check the THN stats for\n","Kansas City, you'll find that Larry has been ...\n","\n","\u001b[1mDocumento ID 9623:\u001b[0m\n","Categoría: \u001b[92mtalk.politics.mideast\u001b[0m\n","Valor de similaridad: \u001b[93m0.2059\u001b[0m\n","Contenido del documento (primeros 120 caracteres): Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n","                 Prelude to Current Events in Nagorn...\n","\n","<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n","\n","\u001b[1mDocumento original ID: 9678\u001b[0m\n","\n","Categoría del documento original: \u001b[94mrec.sport.hockey\u001b[0m\n","\n","Contenido (primeros 100 caracteres): Aargh!\n","\n","Paul Stewart is the worst and most biased ref. presently in the NHL.\n","He called a total of 4 ...\n","\n","Documentos más similares:\n","\n","\u001b[1mDocumento ID 5781:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.3000\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","No.  Patrick Roy is the reason the game was lost, and Ron Hextall is the\n","reason Quebec won.  Everybody said it would co...\n","\n","\u001b[1mDocumento ID 6839:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.2788\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","I don't buy this at all. Roy was the reason the game was tied... and that\n","would *not* have been the case had Dionne kep...\n","\n","\u001b[1mDocumento ID 2032:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.2735\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n",": Speaking of great players, man-oh-man can Quebec skate.  I haven't seen a\n",": team so potent on the rush in a long time...\n","\n","\u001b[1mDocumento ID 2859:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.2632\u001b[0m\n","Contenido del documento (primeros 120 caracteres): The FLYERS blew a 3-0 lead over the Buffalo Sabres in the second period, but\n","Kevin Dineen's 7th career hat trick powered...\n","\n","\u001b[1mDocumento ID 9988:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.2625\u001b[0m\n","Contenido del documento (primeros 120 caracteres): Greetings!\n","\n","Steve Summers and the Chief were on 48 Hours last night shmoozing\n","sports.  I unfortunately missed it.  Those...\n","\n","<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n","\n","\u001b[1mDocumento original ID: 3642\u001b[0m\n","\n","Categoría del documento original: \u001b[94mtalk.religion.misc\u001b[0m\n","\n","Contenido (primeros 100 caracteres): \n","\n","\n","And organized religion is a religion built from organized values.\n","And Ford Tempo is a Tempo built...\n","\n","Documentos más similares:\n","\n","\u001b[1mDocumento ID 8411:\u001b[0m\n","Categoría: \u001b[92mcomp.graphics\u001b[0m\n","Valor de similaridad: \u001b[93m0.2273\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","...for very small values of six and nine....\n","\n","\u001b[1mDocumento ID 784:\u001b[0m\n","Categoría: \u001b[92malt.atheism\u001b[0m\n","Valor de similaridad: \u001b[93m0.2269\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","Science is the process of modeling the real world based on commonly agreed\n","interpretations of our observations (percept...\n","\n","\u001b[1mDocumento ID 4413:\u001b[0m\n","Categoría: \u001b[92mtalk.religion.misc\u001b[0m\n","Valor de similaridad: \u001b[93m0.2265\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","\n","\n","\n","The first means that some aspect of reality contains objective values.\n","The second means that values are a referenc...\n","\n","\u001b[1mDocumento ID 811:\u001b[0m\n","Categoría: \u001b[92mtalk.religion.misc\u001b[0m\n","Valor de similaridad: \u001b[93m0.1957\u001b[0m\n","Contenido del documento (primeros 120 caracteres): Frank, I tried to mail this but it bounced.  It is fast moving out\n","of t.a scope, but I didn't know if t.a was the only g...\n","\n","\u001b[1mDocumento ID 5655:\u001b[0m\n","Categoría: \u001b[92mtalk.religion.misc\u001b[0m\n","Valor de similaridad: \u001b[93m0.1752\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","Is there any difference in saying \n","\n","\"Absolute Truth exists, but some people think its a lie\"\n","\n","and\n","\n","\"Truth is relative...\n","\n","<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n","\n","\u001b[1mDocumento original ID: 2111\u001b[0m\n","\n","Categoría del documento original: \u001b[94msci.space\u001b[0m\n","\n","Contenido (primeros 100 caracteres): \n","\n","\n","Yes, and I do everyone else.  Why, you may wonder, don't I do 'Fred'?\n","Well, that would just be to...\n","\n","Documentos más similares:\n","\n","\u001b[1mDocumento ID 3626:\u001b[0m\n","Categoría: \u001b[92msci.space\u001b[0m\n","Valor de similaridad: \u001b[93m0.5401\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","You missed something.  I think it takes off vertically and is intended\n","to land the same way.\n","\n","-- \n","\"Insisting on perfec...\n","\n","\u001b[1mDocumento ID 5052:\u001b[0m\n","Categoría: \u001b[92msci.space\u001b[0m\n","Valor de similaridad: \u001b[93m0.4050\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","But Allen, if you can assume the existence of an SSTO there is no need\n","to have the contest in the first place.  I wou...\n","\n","\u001b[1mDocumento ID 1680:\u001b[0m\n","Categoría: \u001b[92msci.space\u001b[0m\n","Valor de similaridad: \u001b[93m0.3903\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","\n","Careful.  Making statements about how solid state is (generally) more\n","reliable than analog will get you a nasty foll...\n","\n","\u001b[1mDocumento ID 3018:\u001b[0m\n","Categoría: \u001b[92msci.space\u001b[0m\n","Valor de similaridad: \u001b[93m0.3522\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","If not for the lack of extraneously capitalized words, I'd swear that\n","McElwaine had changed his name and moved to Cal ...\n","\n","\u001b[1mDocumento ID 9227:\u001b[0m\n","Categoría: \u001b[92msci.space\u001b[0m\n","Valor de similaridad: \u001b[93m0.3431\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","American, perhaps, but nothing military about it.  I learned (mostly)\n","slugs when we talked English units in high scho...\n","\n","<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n","\n","\u001b[1mDocumento original ID: 9077\u001b[0m\n","\n","Categoría del documento original: \u001b[94mrec.sport.baseball\u001b[0m\n","\n","Contenido (primeros 100 caracteres): \n","so you want to decrease players' salaries?\n","\n","so you want to increase owners' salaries?\n","\n","the two are ...\n","\n","Documentos más similares:\n","\n","\u001b[1mDocumento ID 8075:\u001b[0m\n","Categoría: \u001b[92msoc.religion.christian\u001b[0m\n","Valor de similaridad: \u001b[93m0.1827\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","If you want to live with someone, you can.\n","If you don't want to have a civil marriage, don't.\n","If you don't want to have...\n","\n","\u001b[1mDocumento ID 9625:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.1596\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","Gee, you'd think Winnipeg would be tops on that list, what with 8 regulars\n","being European.\n","\n","\n","\n","Well, being a Jet fan, I ...\n","\n","\u001b[1mDocumento ID 136:\u001b[0m\n","Categoría: \u001b[92mrec.sport.baseball\u001b[0m\n","Valor de similaridad: \u001b[93m0.1567\u001b[0m\n","Contenido del documento (primeros 120 caracteres): know \n","he \n","now.\n","\n","And Michael Jackson, Jack Nicholson, and Bill Cosby wouldn't be \n","making near as much money if they weren...\n","\n","\u001b[1mDocumento ID 1898:\u001b[0m\n","Categoría: \u001b[92mrec.sport.hockey\u001b[0m\n","Valor de similaridad: \u001b[93m0.1434\u001b[0m\n","Contenido del documento (primeros 120 caracteres): \n","\n","\n","\n","\tI think that you are incorrect, Roger.  Patrick,\n","Smythe and Adams all played or coached in the league before becomi...\n","\n","\u001b[1mDocumento ID 2938:\u001b[0m\n","Categoría: \u001b[92mrec.autos\u001b[0m\n","Valor de similaridad: \u001b[93m0.1433\u001b[0m\n","Contenido del documento (primeros 120 caracteres): I want to start of list for Syclone and Typhoon owners.  If you are interested\n","in participating, please contact me via e...\n","\n","<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n"]}]},{"cell_type":"markdown","source":["# 2. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test.\n","\n"," Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."],"metadata":{"id":"5nSaUihILwI4"}},{"cell_type":"code","source":["# Importar las librerías necesarias\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","\n","# Cargar el dataset \"20 newsgroups\" (subset train)\n","newsgroups_train = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n","\n","# Instanciar el vectorizador TF-IDF\n","tfidfvect = TfidfVectorizer()\n","\n","# Fit-transformar los datos de entrenamiento\n","X_train = tfidfvect.fit_transform(newsgroups_train.data)\n","\n","# Guardar las etiquetas de clasificación\n","y_train = newsgroups_train.target\n","\n","# Separar el conjunto de entrenamiento en conjuntos de entrenamiento y validación\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# Entrenar y evaluar modelos Naïve Bayes\n","models = {\n","    \"MultinomialNB\": MultinomialNB(),\n","    \"ComplementNB\": ComplementNB()\n","}\n","\n","for model_name, model in models.items():\n","    # Entrenar el modelo\n","    model.fit(X_train, y_train)\n","\n","    # Realizar predicciones sobre el conjunto de validación\n","    y_pred = model.predict(X_val)\n","\n","    # Calcular el F1-score macro\n","    f1 = f1_score(y_val, y_pred, average=\"macro\")\n","\n","    # Imprimir el F1-score\n","    print(f\"{model_name} F1 Score: {f1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpF2LphYinEm","executionInfo":{"status":"ok","timestamp":1728381428096,"user_tz":180,"elapsed":4836,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"e80c6ae3-f508-493e-9509-c7b66fc21989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MultinomialNB F1 Score: 0.6200\n","ComplementNB F1 Score: 0.7493\n"]}]},{"cell_type":"markdown","source":["\n","\n","\n","# **3**. Transponer la matriz documento-término.\n","De esa manera se obtiene una matriz\n","término-documento que puede ser interpretada como una colección de vectorización de palabras.\n","Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."],"metadata":{"id":"dWBERjF_ktPp"}},{"cell_type":"code","source":["\n","# Transponer la matriz documento-término para obtener una matriz término-documento\n","X_train_transposed = X_train.T\n","\n","# Obtener el vocabulario\n","vocabulary = tfidfvect.vocabulary_\n","\n","# Mostrar algunas palabras del vocabulario\n","words = list(vocabulary.keys()) # Convierte el conjunto de claves del diccionario vocabulary en una lista de palabras\n","print(f\"Cantidad total de palabras en el vocabulario: {len(words)}\") # Imprime el número total de palabras en el vocabulario\n","print(\"Primeras palabras en el vocabulario:\", words[:100],\"\\n\")  # Muestra las primeras 20 palabras\n","\n","# Elegir manualmente 5 palabras para evaluar su similaridad\n","palabras = [\"name\", \"door\", \"made\", \"mail\", \"number\"]\n","\n","# Crear un diccionario inverso para mapear índices a palabras\n","idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n","\n","# Calcular y mostrar las palabras más similares para cada palabra elegida\n","for palabra in palabras:\n","    if palabra in tfidfvect.vocabulary_:\n","        # Obtener el índice de la palabra en la matriz transpuesta\n","        idx = tfidfvect.vocabulary_[palabra]\n","\n","        # Obtener el vector de la palabra seleccionada\n","        palabra_vect = X_train_transposed[idx]\n","\n","        # Calcular la similaridad coseno entre la palabra seleccionada y todas las demás\n","        similar_words = cosine_similarity(palabra_vect, X_train_transposed)[0]\n","\n","        # Obtener los 5 índices de palabras más similares (excluyendo la palabra misma)\n","        most_similar_idx = np.argsort(similar_words)[::-1][1:6]\n","\n","        # Mostrar los resultados\n","        print(f\"\\033[1mPalabra elegida: {palabra}\\033[0m\")\n","        for idx in most_similar_idx:\n","            print(f\"Palabra similar encontrada: {idx2word[idx]}\")\n","        print(\"-\" * 70)\n","    else:\n","        print(f\"Palabra \"{palabra}\" no encontrada en el vocabulario.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17AI_jOVmXWO","executionInfo":{"status":"ok","timestamp":1728382857515,"user_tz":180,"elapsed":1053,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"9299041d-bd0c-4e6e-d842-d1f4a6f98573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cantidad total de palabras en el vocabulario: 101631\n","Primeras palabras en el vocabulario: ['was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'door', 'sports', 'looked', 'to', 'be', 'from', 'late', '60s', 'early', '70s', 'called', 'bricklin', 'doors', 'were', 'really', 'small', 'in', 'addition', 'front', 'bumper', 'separate', 'rest', 'of', 'body', 'is', 'all', 'know', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'where', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'funky', 'looking', 'please', 'mail', 'fair', 'number', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'for', 'poll', 'send', 'brief', 'message', 'detailing', 'your', 'with', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'add', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'per', 'floppy'] \n","\n","\u001b[1mPalabra elegida: name\u001b[0m\n","Palabra similar encontrada: sucb\n","Palabra similar encontrada: router\n","Palabra similar encontrada: fw\n","Palabra similar encontrada: unofficial\n","Palabra similar encontrada: the\n","----------------------------------------------------------------------\n","\u001b[1mPalabra elegida: door\u001b[0m\n","Palabra similar encontrada: secluded\n","Palabra similar encontrada: openers\n","Palabra similar encontrada: normaly\n","Palabra similar encontrada: genie\n","Palabra similar encontrada: prelude\n","----------------------------------------------------------------------\n","\u001b[1mPalabra elegida: made\u001b[0m\n","Palabra similar encontrada: the\n","Palabra similar encontrada: that\n","Palabra similar encontrada: of\n","Palabra similar encontrada: to\n","Palabra similar encontrada: and\n","----------------------------------------------------------------------\n","\u001b[1mPalabra elegida: mail\u001b[0m\n","Palabra similar encontrada: please\n","Palabra similar encontrada: me\n","Palabra similar encontrada: thanks\n","Palabra similar encontrada: address\n","Palabra similar encontrada: for\n","----------------------------------------------------------------------\n","\u001b[1mPalabra elegida: number\u001b[0m\n","Palabra similar encontrada: dial\n","Palabra similar encontrada: phone\n","Palabra similar encontrada: serial\n","Palabra similar encontrada: the\n","Palabra similar encontrada: of\n","----------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["\n","\n","\n","# **4**. Prueba ajustando parámetros de instanciación del vectorizador y modelos\n","Para explorar cómo los parámetros de instanciación del vectorizador y los modelos afectan el desempeño del modelo. Aquí lo que se probó fue:\n","1. Modificar parámetros del TfidfVectorizer, ya que tiene varios parámetros que pueden influir en el desempeño del modelo, tales como:\n","- max_features: limitar el número máximo de términos (palabras).\n","- ngram_range: cambiar el rango de n-gramas (por ejemplo, usar bigramas).\n","- max_df y min_df: para eliminar palabras demasiado comunes o demasiado raras.\n","2. Modificar parámetros de los modelos Naïve Bayes, tanto MultinomialNB como ComplementNB tienen el parámetro:\n","\n","- alpha: para suavizar las probabilidades y evitar que los valores sean cero (suavizado de Laplace)."],"metadata":{"id":"vLfVJW26t9gx"}},{"cell_type":"code","source":["# Importar las librerías necesarias\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","\n","# Cargar el dataset \"20 newsgroups\" (subset train)\n","newsgroups_train = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n","\n","# Configuración del vectorizador 1: Unigrams con 5000 términos\n","tfidfvect1 = TfidfVectorizer(max_features=5000, ngram_range=(1, 1))\n","\n","# Configuración del vectorizador 2: Unigrams + Bigrams con 5000 términos\n","tfidfvect2 = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n","\n","# Configuración del vectorizador 3: Solo unigrams con 10000 términos\n","tfidfvect3 = TfidfVectorizer(max_features=10000, ngram_range=(1, 1))\n","\n","# Lista de vectorizadores a probar\n","vectorizers = [tfidfvect1, tfidfvect2, tfidfvect3]\n","\n","# Modelos Naïve Bayes\n","multinomial_nb1 = MultinomialNB(alpha=1.0)  # Multinomial con alpha=1\n","multinomial_nb2 = MultinomialNB(alpha=0.5)  # Multinomial con alpha=0.5\n","complement_nb1 = ComplementNB(alpha=1.0)    # Complement con alpha=1\n","complement_nb2 = ComplementNB(alpha=0.5)    # Complement con alpha=0.5\n","\n","# Lista de modelos a probar\n","models = [multinomial_nb1, multinomial_nb2, complement_nb1, complement_nb2]\n","\n","# Entrenar y evaluar combinaciones de vectorizadores y modelos\n","for i, vectorizer in enumerate(vectorizers, start=1):\n","    print(f\"\\nProbando vectorizador {i}...\\n\")\n","\n","    # Fit-transformar los datos de entrenamiento\n","    X_train = vectorizer.fit_transform(newsgroups_train.data)\n","\n","    # Guardar las etiquetas de clasificación\n","    y_train = newsgroups_train.target\n","\n","    # Separar el conjunto de entrenamiento en conjuntos de entrenamiento y validación\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","    # Entrenar y evaluar cada modelo Naïve Bayes\n","    for x, model in enumerate(models, start=1):\n","        # Entrenar el modelo\n","        model.fit(X_train, y_train)\n","\n","        # Realizar predicciones sobre el conjunto de validación\n","        y_pred = model.predict(X_val)\n","\n","        # Calcular el F1-score macro\n","        f1 = f1_score(y_val, y_pred, average=\"macro\")\n","\n","        # Imprimir el F1-score\n","        model_name = f\"MultinomialNB {x}\" if x <= 2 else f\"ComplementNB {x-2}\"\n","        print(f\"{model_name} con Vectorizador {i} F1 Score: {f1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80L7ophEwo7K","executionInfo":{"status":"ok","timestamp":1728384795974,"user_tz":180,"elapsed":18686,"user":{"displayName":"marcos gallo","userId":"10601798968189464567"}},"outputId":"78640b3f-f1d6-45d8-ad26-b456b1a075a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Probando vectorizador 1...\n","\n","MultinomialNB 1 con Vectorizador 1 F1 Score: 0.6448\n","MultinomialNB 2 con Vectorizador 1 F1 Score: 0.6717\n","ComplementNB 1 con Vectorizador 1 F1 Score: 0.6827\n","ComplementNB 2 con Vectorizador 1 F1 Score: 0.6827\n","\n","Probando vectorizador 2...\n","\n","MultinomialNB 1 con Vectorizador 2 F1 Score: 0.6150\n","MultinomialNB 2 con Vectorizador 2 F1 Score: 0.6295\n","ComplementNB 1 con Vectorizador 2 F1 Score: 0.6380\n","ComplementNB 2 con Vectorizador 2 F1 Score: 0.6362\n","\n","Probando vectorizador 3...\n","\n","MultinomialNB 1 con Vectorizador 3 F1 Score: 0.6518\n","MultinomialNB 2 con Vectorizador 3 F1 Score: 0.6891\n","ComplementNB 1 con Vectorizador 3 F1 Score: 0.7180\n","ComplementNB 2 con Vectorizador 3 F1 Score: 0.7144\n"]}]},{"cell_type":"markdown","source":["## Proyecto de análisis de texto\n","Se utiliza como base el conjunto de datos \"20 Newsgroups\", que contiene documentos de texto distribuidos en diversas categorías temáticas . El objetivo principal fue procesar este conjunto de datos utilizando técnicas de vectorización de texto, de cálculo de similitud entre documentos y de entrenamiento de modelos de clasificación para analizar luego su rendimiento.\n","\n","\n","Para convertir los textos en representaciones numéricas, se utilizó el vectorizador TF-IDF (Term Frequency-Inverse Document Frequency). Este método transforma los textos en una matriz esparsa, donde cada fila corresponde a un documento y cada columna a un término del vocabulario. En este caso, se generó un vocabulario de 101,631 palabras, lo que indica un amplio rango de términos presentes en el conjunto de datos.\n","\n","Para lograr una mejor comprensión y eficiencia en la lectura de datos, se eliminaron elementos no relevantes como encabezados, pies de página y citas, lo que permite concentrarse exclusivamente en el contenido relevante de cada documento.\n","\n","\n","Hecho esto, se analiza la similitud entre documentos, seleccionando al azar 5 documentos del conjunto de datos y calculando su \"similaridad coseno\" con el resto. Este cálculo permite medir cuán parecidos son dos documentos en cuanto a los términos que comparten.\n","\n","A partir de los resultados, se identificaron los 5 documentos más similares para cada uno de los seleccionados. Se imprimieron las categorías de los documentos, su índice, la similaridad y un fragmento de su contenido, lo que proporciona una visión general del texto.\n","\n","## Entrenamiento de modelos de clasificación\n","Una parte clave del análisis fue entrenar modelos de clasificación para predecir las categorías de los documentos. Se trabajó con dos algoritmos de Naive Bayes:\n","\n","- MultinomialNB\n","- ComplementNB\n","\n","Estos modelos se entrenaron utilizando el conjunto de datos transformado mediante TF-IDF, y su rendimiento se evaluó utilizando el F1-score macro. Este tipo de métrica permite evaluar el rendimiento global de los modelos teniendo en cuenta tanto la precisión como la exhaustividad para cada clase.\n","\n","### Resultados de los modelos\n","MultinomialNB obtuvo un F1-score de 0.5854, es decir un rendimiento medio.\n","ComplementNB logró un F1-score de 0.6930, mejorando el desempeño en comparación con el modelo anterior.\n","El mejor rendimiento del modelo ComplementNB sugiere que es más adecuado para manejar clases desbalanceadas en este conjunto de datos, una situación común en problemas de clasificación de texto.\n","\n","### Análisis de similaridad de términos\n","Además de analizar documentos, se llevó a cabo un estudio de similaridad de palabras. Se transpuso la matriz documento-término para obtener una matriz término-documento, lo que permitió calcular la similaridad coseno entre las palabras del vocabulario.\n","\n","Se eligieron 5 palabras (\"name\", \"door\", \"made\", \"mail\", \"number\") para evaluar su similaridad con otras palabras. Para cada una de ellas, se identificaron las 5 palabras más similares, lo que permite explorar relaciones entre términos dentro del corpus de documentos.\n","\n","## Ajuste de parámetros de instanciación\n","El ajuste de los parámetros tanto en los vectorizadores como en los modelos Naive Bayes influye de manera significativa en el rendimiento del sistema de clasificación.\n","En la configuración del Vectorizador TF-IDF los parámetros ajustados en los diferentes vectorizadores fueron:\n","- Número máximo de términos (max_features) con el que podemos limitar o ampliar el tamaño de las representaciones de texto.\n","- Rango de N-gramas (ngram_range)que define si el vectorizador considera solo palabras individuales (unigrams) o también combinaciones de palabras (bigrams).Los unigrams tienden a capturar información semántica básica, mientras que los bigrams pueden captar patrones o relaciones entre palabras.\n","\n","En la configuración de los Modelos Naive Bayes\n","el parámetro clave ajustado fue:\n","\n","- alpha: El parámetro de suavización de Laplace. Este parámetro controla cómo se manejan las características (palabras) que no están presentes en una clase determinada. Un valor de alpha=1 implica mayor suavización, mientras que valores menores (como alpha=0.5) permiten que las probabilidades estén más influenciadas por las frecuencias observadas.\n","\n","## **Conclusiones personales**\n","Este proyecto en mi caso me obligó a la búsqueda en línea de material (mayormente videos) que me terminen de aclarar el tema, ya que no tenía ninguna experiencia previa. He visto que las técnicas de Procesamiento de Lenguaje Natural que comenzamos a utilizar necesitan de mucha prueba y ajuste para lograr un resultado eficiente y depende en gran medida de la estructura del texto a analizar.\n","\n","Viendo como ComplementNB supera a MultinomialNB en este conjunto de datos donde las clases no están balanceadas, se nota la importancia de seleccionar el algoritmo adecuado para mejorar el rendimiento en la clasificación de textos.\n","\n","El ajuste de los parámetros del vectorizador y de los modelos de Naive Bayes impacta  en el rendimiento del sistema de clasificación. En este caso el ajuste del parámetro Alpha en la instanciación de modelos no afectó prácticamente al rendimiento, si lo hizo el ajuste del vectorizador, donde utilizar sólo unigrams con un mayor número de términos ayudó a capturar más información contextual del texto, mejorando la precisión.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"oi3ZsawxESMf"}}]}