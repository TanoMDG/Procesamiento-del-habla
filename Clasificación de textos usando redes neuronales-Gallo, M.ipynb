{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONSIGNAS QUE DEBE RESPONDER\n",
        "\n",
        "1) Describa las principales características del dataset utilizado.\n",
        "\n",
        "2) Deben comentar su experiencia al intentar correr el mismo código en sus computadoras: cuáles errores surgieron, cómo los solucionaron, sugerencias que brindan, si debieron o no modificar algo del código para solucionar algún error o poder correr el ejemplo en sus máquinas.\n",
        "\n",
        "3) Explique con sus palabras el pre-procesamiento que realiza en el dataset para poder utilizarlo para entrenar la red neuronal\n",
        "\n",
        "Postee también el resultado final de evaluación del modelo ( la salida del apartado \"Evaluate the model on the test set)\n",
        "\n",
        "4) En este ejemplo para evaluar el modelo utiliza la métrica accuracy. ¿Qué opina al respecto? ¿Está bien/mal? ¿sumaría otra métrica?\n",
        "\n",
        "5) Comente alguna parte que le haya llamado la atención y por qué.\n",
        "\n",
        "6) Escriba las dudas que hayan quedado de realizar este trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RESPUESTA 1) Describa las principales características del dataset utilizado.\n",
        "\n",
        "El dataset que se utilizó, el IMDB Movie Review Dataset, como lo explica en el archivo Readme, consiste en un conjunto de datos que contiene reseñas de películas junto con sus etiquetas de polaridad de sentimiento binario asociadas, indicando así el sentimiento del texto. Está diseñado para servir como un punto de referencia para la clasificación de sentimientos.\n",
        "\n",
        "En cuanto al número de muestras: \n",
        "\n",
        "El conjunto de datos principal contiene 50,000 reseñas divididas equitativamente en 25,000 para entrenamiento y 25,000 para pruebas. Se encuentra balanceada la distribución general de las etiquetas (25,000 positivas y 25,000 negativas). Incluye además 50,000 documentos adicionales sin etiquetar para aprendizaje no supervisado.\n",
        "\n",
        "\n",
        "El formato de los datos de las reseñas es de un texto sin procesar. Esto significa que cada muestra es una secuencia de palabras con longitud variable, y no están preprocesadas (por ejemplo, sin tokenización o eliminación de palabras irrelevantes).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RESPUESTA 2) Deben comentar su experiencia al intentar correr el mismo código en sus computadoras: cuáles errores surgieron, cómo los solucionaron, sugerencias que brindan, si debieron o no modificar algo del código para solucionar algún error o poder correr el ejemplo en sus máquinas.\n",
        "\n",
        "1)   El **primer error** que surgió fue: \"ModuleNotFoundError: No module named 'keras'\". Lo que pasaba es que no tenía instalada la biblioteca \"Keras\", la instalé entonces con el comando \"!pip install keras\".\n",
        "\n",
        "2)   EL **segundo error** fue: \"ModuleNotFoundError: No module named 'tensorflow'\", por lo que procedí a instalar tensorflow, la biblioteca de código abierto desarrollada por Google para el aprendizaje automático con el comando \"!pip install tensorflow\".\n",
        "\n",
        "3)   EL **tercer error** surgió al intentar ejecutar el comando \"!ls aclImdb\", donde apareció la frase: \"\"ls\" no se reconoce como un comando interno o externo,\n",
        "programa o archivo por lotes ejecutable\". Indica que se está intentando ejecutar el comando ls en un sistema operativo que no lo reconoce, Windows. En Windows, el comando ls no está disponible de forma predeterminada; en su lugar se utiliza el comando dir para listar los archivos y directorios. [!dir aclImdb].\n",
        "\n",
        "Otra solución aquí sería, para poder usar comandos de Unix como ls, utilizar el entorno WSL (Windows Subsystem for Linux) o alguna terminal que soporte comandos de Unix, pero esto requiere tener WSL instalado y configurado.\n",
        "\n",
        "También es interesante la opción de usar Python para listar los archivos en el directorio de la siguiente manera:\n",
        "\n",
        "    import os\n",
        "\n",
        "    #Lista el contenido del directorio aclImdb\n",
        "\n",
        "    print(os.listdir(\"aclImdb\"))\n",
        "\n",
        "Pasó una situación similar al intentar acceder a los directorios dentro de las carpetas train y test, por lo que sustituí los comandos por:\n",
        "\n",
        "    print(os.listdir(\"aclImdb/train\"))\n",
        "\n",
        "    print(os.listdir(\"aclImdb/test\"))\n",
        "\n",
        "4)  El **cuarto error**: \"cat\" no se reconoce como un comando interno o externo, programa o archivo por lotes ejecutable fue también porque el comando cat es nativo de sistemas operativos basados en Unix (Linux, MacOS), pero no está disponible en Windows de forma nativa.\n",
        "\n",
        "Como solución puedo utilizar el comando type que es el equivalente en windows o leer el archivo con python, que fue lo que hice:\n",
        "\n",
        "    with open('aclImdb/train/pos/6248_7.txt', 'r') as file:\n",
        "\n",
        "       content = file.read()\n",
        "\n",
        "       print(content)\n",
        "\n",
        "5)   El **quinto error** fue: \"rm\" no se reconoce como un comando interno o externo, programa o archivo por lotes ejecutable. \n",
        "\n",
        "En este caso el comando equivalente en windos es \"rmdir /s /q\", sino se puede eliminar usando python así:\n",
        "\n",
        "    import shutil\n",
        "\n",
        "    # Ruta al directorio que deseas eliminar\n",
        "    dir_path = 'aclImdb/train/unsup'\n",
        "\n",
        "    # Elimina el directorio y su contenido\n",
        "    shutil.rmtree(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RESPUESTA 3)   Explique con sus palabras el pre-procesamiento que realiza en el dataset para poder utilizarlo para entrenar la red neuronal\n",
        "\n",
        "Las reseñas, que están en formato de texto sin procesar, tienen que ser comprendidas por una red neuronal, para esto durante el pre-proceamientoe de datos se realizan varios cambios:\n",
        "\n",
        "Primero, se cargan los datos desde el disco, organizando los archivos de texto en comjuntos separados para entrenamiento (donde se aparta 80% para entrenamiento y 20% para validación) y prueba. Se cargan desde las carpetas Train y Test usando funciones de Keras.\n",
        "\n",
        "Para la limpieza y estandarización se crea una función personalizada que convierte el texto a minúsculas, elimina etiquetas HTML y quita la puntuación. El modelo elimina las puntuaciones, convierte las palabras a minúsculas y divide las oraciones en palabras para crear secuencias que puedan ser procesadas.\n",
        "Se establecen algunas constantes como la cantidad máxima de palabras únicas que tendrá el modelo en cuenta (20000), la dimensión de los embeddings de las palabras (128) y una longitud fija de palabras del texto (500), truncando las que sobrepasen las 500 y rellenando con ceros las que tengan menos.\n",
        "Se usa la capa TextVectorization de Keras, que convierte el texto en tokens, es decir, separa las palabras del texto en su forma básica. Se convierte cada palabra en un número único (índice) que representa su posición en un vocabulario.\n",
        "\n",
        "Se generan los vectores después de tokenizar el texto, las palabras se transforman en secuencias de índices numéricos. Cada índice corresponde a una palabra en un diccionario creado durante el entrenamiento. La secuencia de palabras de cada reseña se convierte en una secuencia de enteros.\n",
        "\n",
        "Se crea un conjunto de datos que solo contiene el texto, ignorando las etiquetas. Se aplica una función a los conjuntos de entrenamiento, validación y prueba para ignorar la segunda parte de cada par (texto, etiqueta) y que luego devuelve solo el texto, quedando las etiquetas intactas.\n",
        "\n",
        "Se utiliza también la creación de lotes, ya que las reseñas procesadas se dividen en lotes que son alimentados a la red neuronal durante el entrenamiento, con esto se logra entrenar el modelo en pequeñas porciones del dataset, lo que permite mejorar el rendimiento.\n",
        "\n",
        "Luego de la creación y entrenamiento del modelo, este se evalúa utilizando el conjunto de prueba (test set).\n",
        "El resultado que se obtiene es el valor de la métrica de evaluación, en este caso accuracy. \n",
        "La exactitud es una métrica que refleja la proporción de predicciones correctas, es decir, cuántas reseñas fueron clasificadas correctamente como positivas o negativas.\n",
        "\n",
        "El resultado en este proceso fue de:\n",
        "\n",
        "- accuracy: 0.8568 \n",
        "- loss: 0.4563\n",
        "\n",
        "Resultado con el agregado de otras métricas:\n",
        "\n",
        "- AUC: 0.9351 (Excelente capacidad para distinguir entre reseñas positivas y negativas)\n",
        "- Precision: 0.8358 ( Buen nivel de precisión, hay sólo 16,42% de falsos positivos que podrían mejorarse)\n",
        "- Recall: 0.8954 (Excelente capacidad para identificar reseñas positivas. El modelo casi no deja pasar reseñas positivas sin detectarlas.)\n",
        "- accuracy: 0.8598 (Buen rendimiento general, sólo alrededor del 14.02% de las predicciones mal clasificadas.)\n",
        "- loss: 0.3812 (Error moderado en las predicciones, lo que sugiere que el modelo está ajustado de manera confiable.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) En este ejemplo para evaluar el modelo utiliza la métrica accuracy. ¿Qué opina al respecto? ¿Está bien/mal? ¿sumaría otra métrica?\n",
        "\n",
        "Cre que el uso de la métrica accuracy es apropiado para este problema debido a la naturaleza balanceada del dataset (50% reseñas positivas, 50% negativas). En este caso, la exactitud es una buena indicación del rendimiento general del modelo. \n",
        "\n",
        "Esta métrica mide la proporción de predicciones correctas del modelo en comparación con el total de predicciones realizadas.\n",
        "\n",
        "Además de la métrica de accuracy (exactitud), la métrica que podría ser útil agregar es la Precisión. Esta mide la proporción de predicciones positivas correctas sobre el total de predicciones positivas realizadas por el modelo. Nos da información adicional sobre cómo maneja los falsos positivos. Aunque el accuracy diga que el 85.9% de las predicciones fueron correctas, no informa sobre cuántos de los ejemplos positivos predichos fueron realmente correctos. En un escenario como el análisis de sentimientos, si el objetivo es evitar clasificar erróneamente reseñas negativas como positivas (como este ejemplo de las reseñas de películas), la precisión nos importaria.\n",
        "\n",
        "Por ejemplo, si en el sistema de clasificación de reseñas de películas se clasifica incorrectamente una reseña negativa como positiva, y el sistema no puede identificar correctamente la opinión negativa, puede llevar a malas decisiones. Aquí es un caso donde se necesita evitar los falsos positivos.\n",
        "\n",
        "Hay también otras métricas que podrían ofrecer una visión más detallada del rendimiento del modelo:\n",
        "\n",
        "Recall (Sensibilidad): El recall es importante para entender cuántos de los verdaderos ejemplos positivos fueron correctamente identificados. En el escenario donde interese capturar todas las reseñas negativas (por ejemplo, para detectar películas que no fueron lo que esperaba el público), un alto recall sería crucial, incluso si la accuracy general es alta.\n",
        "\n",
        "AUC (Área bajo la curva ROC): Aunque el accuracy puede ser alto, el AUC-ROC serviría para evaluar la capacidad del modelo para distinguir correctamente entre reseñas positivas y negativas en un rango de umbrales. Sería útil si luego se piensa ajustar el umbral de clasificación para optimizar algún objetivo específico, como maximizar la precisión en lugar del recall. \n",
        "\n",
        "Creo que puede ser útil agregar estas métricas que nos permitan evaluar el rendimiento en aspectos más específicos, medir la capacidad para distinguir entre clases con el AUC y ver algún problema oculto (como un sesgo en la clasificación) que el accuracy no refleje.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RESPUESTA 5) Comente alguna parte que le haya llamado la atención y por qué.\n",
        "\n",
        "Si bien el análisis del trabajo tuvo muchas partes que son nuevas para lo que hasta ahora conozco de NPL, me llamó la atención el uso de la capa TextVectorization para la tokenización y vectorización de las palabras.  Esta capa dentro del modelo facilita todo el proceso y lo hace más eficiente. \n",
        "\n",
        "También la parte del final del trabajo donde se ve la posibilidad de construir un modelo completo que pueda recibir texto crudo y realizar predicciones sin requerir preprocesamiento externo, parece muy poderoso para la implementación práctica en producción. Este enfoque también elimina la necesidad de almacenar los mismos pasos de preprocesamiento por separado y permite que el modelo maneje todo la secuencia de pasos de principio a fin. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Escriba las dudas que hayan quedado de realizar este trabajo.\n",
        "\n",
        "Durante la ejecución del código en mi máquina, las dudas que me fueron surgiendo las fui aclarando buscando material en línea. Lo que sí no me quedó del todo claro fue ¿qué métricas aplicar en el caso de la evaluación del modelo?, en el sentido de que si las clases estan perfectamente balanceadas como era el caso de este trabajo, sé que la métrica \"accuracy\" es representativa del rendimiento del modelo. Pero siempre se puede utilizar las otras métricas como \"recall\", \"precision\", \"AUC\" para poder tener un análisis más profundo. Entoces, ¿podrían usarse siempre estas otras métricas que nos den informacción sobre el desempeño del modelo? Siempre sabiendo luego interpretar los resultados. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **A CONTINUACIÓN CORRO EL CÓDIGO EN MI MÁQUINA Y MODIFICO ALGUNAS LÍNEAS DE CÓDIGO PARA PODER EJECUTARLO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DS4VFPj1ffs"
      },
      "source": [
        "# Text classification from scratch\n",
        "\n",
        "**Authors:** Mark Omernick, Francois Chollet<br>\n",
        "**Date created:** 2019/11/06<br>\n",
        "**Last modified:** 2020/05/17<br>\n",
        "**Description:** Text sentiment classification starting from raw text files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQoP2cAa1ffx"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example shows how to do text classification starting from raw text (as\n",
        "a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n",
        "classification dataset (unprocessed version). We use the `TextVectorization` layer for\n",
        " word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppdq4MKH1ffy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oAQMXlJe1ffz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: absl-py in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (2.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (1.24.2)\n",
            "Requirement already satisfied: rich in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (13.9.2)\n",
            "Requirement already satisfied: namex in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras) (2.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gallo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lerExNsO1ff0"
      },
      "source": [
        "## Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4cPb6QKD1ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 80.2M    0 49152    0     0  30055      0  0:46:39  0:00:01  0:46:38 30117\n",
            "  0 80.2M    0  256k    0     0  99459      0  0:14:05  0:00:02  0:14:03 99598\n",
            "  0 80.2M    0  720k    0     0   198k      0  0:06:54  0:00:03  0:06:51  198k\n",
            "  2 80.2M    2 2144k    0     0   463k      0  0:02:57  0:00:04  0:02:53  463k\n",
            "  5 80.2M    5 4528k    0     0   806k      0  0:01:41  0:00:05  0:01:36  909k\n",
            "  9 80.2M    9 7616k    0     0  1150k      0  0:01:11  0:00:06  0:01:05 1518k\n",
            " 13 80.2M   13 11.1M    0     0  1496k      0  0:00:54  0:00:07  0:00:47 2237k\n",
            " 19 80.2M   19 15.5M    0     0  1848k      0  0:00:44  0:00:08  0:00:36 3052k\n",
            " 25 80.2M   25 20.7M    0     0  2212k      0  0:00:37  0:00:09  0:00:28 3833k\n",
            " 33 80.2M   33 26.5M    0     0  2563k      0  0:00:32  0:00:10  0:00:22 4535k\n",
            " 41 80.2M   41 33.5M    0     0  2962k      0  0:00:27  0:00:11  0:00:16 5364k\n",
            " 49 80.2M   49 39.7M    0     0  3223k      0  0:00:25  0:00:12  0:00:13 5840k\n",
            " 58 80.2M   58 47.2M    0     0  3548k      0  0:00:23  0:00:13  0:00:10 6468k\n",
            " 67 80.2M   67 54.0M    0     0  3788k      0  0:00:21  0:00:14  0:00:07 6816k\n",
            " 76 80.2M   76 61.1M    0     0  4013k      0  0:00:20  0:00:15  0:00:05 7091k\n",
            " 85 80.2M   85 68.6M    0     0  4232k      0  0:00:19  0:00:16  0:00:03 7181k\n",
            " 94 80.2M   94 76.0M    0     0  4420k      0  0:00:18  0:00:17  0:00:01 7458k\n",
            "100 80.2M  100 80.2M    0     0  4470k      0  0:00:18  0:00:18 --:--:-- 7115k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYVZct9W1ff1"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OvLgzZTA1ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " El volumen de la unidad C es Windows\n",
            " El n�mero de serie del volumen es: 84E7-5C86\n",
            "\n",
            " Directorio de c:\\Users\\Marcos\\Ciencia datos e inteligencia artificial\\CUARTO SEMESTRE\\PROCESAMIENTO DEL HABLA\\aclImdb\n",
            "\n",
            "13/10/2024  20:37    <DIR>          .\n",
            "13/10/2024  01:17    <DIR>          ..\n",
            "12/04/2011  14:14           845.980 imdb.vocab\n",
            "11/06/2011  19:54           903.029 imdbEr.txt\n",
            "25/06/2011  21:18             4.037 README\n",
            "13/10/2024  20:37    <DIR>          test\n",
            "13/10/2024  20:37    <DIR>          train\n",
            "               3 archivos      1.753.046 bytes\n",
            "               4 dirs  326.631.768.064 bytes libres\n",
            "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']\n"
          ]
        }
      ],
      "source": [
        "!dir aclImdb\n",
        "# Lista el contenido del directorio aclImdb\n",
        "print(os.listdir(\"aclImdb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "umN739_o1ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['labeledBow.feat', 'neg', 'pos', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"aclImdb/train\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kb9CtO4C1ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['labeledBow.feat', 'neg', 'pos', 'urls_neg.txt', 'urls_pos.txt']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"aclImdb/test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38K1gKVw1ff3"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
        " which represents one review (either positive or negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Pqa2IJ9N1ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!\n"
          ]
        }
      ],
      "source": [
        "# Leer el contenido del archivo usando Python\n",
        "file_path = 'aclImdb/train/pos/6248_7.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9qSG1Gg1ff4"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the other subfolder that has text files in it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "be6948OK1ff4"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Ruta al directorio a eliminar\n",
        "dir_path = 'aclImdb/train/unsup'\n",
        "\n",
        "# Elimina el directorio y su contenido\n",
        "shutil.rmtree(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjpmVt2l1ff4"
      },
      "source": [
        "You can use the utility `keras.utils.text_dataset_from_directory` to\n",
        "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed\n",
        " into class-specific folders.\n",
        "\n",
        "Let's use it to generate the training, validation, and test datasets. The validation\n",
        "and training datasets are generated from two subsets of the `train` directory, with 20%\n",
        "of samples going to the validation dataset and 80% going to the training dataset.\n",
        "\n",
        "Having a validation dataset in addition to the test dataset is useful for tuning\n",
        "hyperparameters, such as the model architecture, for which the test dataset should not\n",
        "be used.\n",
        "\n",
        "Before putting the model out into the real world however, it should be retrained using all\n",
        "available training data (without creating a validation dataset), so its performance is maximized.\n",
        "\n",
        "When using the `validation_split` & `subset` arguments, make sure to either specify a\n",
        "random seed, or to pass `shuffle=False`, so that the validation & training splits you\n",
        "get have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "D67nRnf51ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqy7Lf8C1ff5"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bdsDjqDl1ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'I\\'ve seen tons of science fiction from the 70s; some horrendously bad, and others thought provoking and truly frightening. Soylent Green fits into the latter category. Yes, at times it\\'s a little campy, and yes, the furniture is good for a giggle or two, but some of the film seems awfully prescient. Here we have a film, 9 years before Blade Runner, that dares to imagine the future as somthing dark, scary, and nihilistic. Both Charlton Heston and Edward G. Robinson fare far better in this than The Ten Commandments, and Robinson\\'s assisted-suicide scene is creepily prescient of Kevorkian and his ilk. Some of the attitudes are dated (can you imagine a filmmaker getting away with the \"women as furniture\" concept in our oh-so-politically-correct-90s?), but it\\'s rare to find a film from the Me Decade that actually can make you think. This is one I\\'d love to see on the big screen, because even in a widescreen presentation, I don\\'t think the overall scope of this film would receive its due. Check it out.'\n",
            "1\n",
            "b'First than anything, I\\'m not going to praise I\\xc3\\xb1arritu\\'s short film, even I\\'m Mexican and proud of his success in mainstream Hollywood.<br /><br />In another hand, I see most of the reviews focuses on their favorite (and not so) short films; but we are forgetting that there is a subtle bottom line that circles the whole compilation, and maybe it will not be so pleasant for American people. (Even if that was not the main purpose of the producers) <br /><br />What i\\'m talking about is that most of the short films does not show the suffering that WASP people went through because the terrorist attack on September 11th, but the suffering of the Other people.<br /><br />Do you need proofs about what i\\'m saying? Look, in the Bosnia short film, the message is: \"You cry because of the people who died in the Towers, but we (The Others = East Europeans) are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you\".<br /><br />Even though the Burkina Fasso story is more in comedy, there is a the same thought: \"You are angry because Osama Bin Laden punched you in an evil way, but we (The Others = Africans) should be more angry, because our people is dying of hunger, poverty and AIDS long time ago, and nobody pay attention to us like the whole world has done to you\".<br /><br />Look now at the Sean Penn short: The fall of the Twin Towers makes happy to a lonely (and alienated) man. So the message is that the Power and the Greed (symbolized by the Towers) must fall for letting the people see the sun rise and the flowers blossom? It is remarkable that this terrible bottom line has been proposed by an American. There is so much irony in this short film that it is close to be subversive.<br /><br />Well, the Ken Loach (very know because his anti-capitalism ideology) is much more clearly and shameless in going straight to the point: \"You are angry because your country has been attacked by evil forces, but we (The Others = Latin Americans) suffered at a similar date something worst, and nobody remembers our grief as the whole world has done to you\".<br /><br />It is like if the creative of this project wanted to say to Americans: \"You see now, America? You are not the only that have become victim of the world violence, you are not alone in your pain and by the way, we (the Others = the Non Americans) have been suffering a lot more than you from long time ago; so, we are in solidarity with you in your pain... and by the way, we are sorry because you have had some taste of your own medicine\" Only the Mexican and the French short films showed some compassion and sympathy for American people; the others are like a slap on the face for the American State, that is not equal to American People.'\n",
            "1\n",
            "b'Blood Castle (aka Scream of the Demon Lover, Altar of Blood, Ivanna--the best, but least exploitation cinema-sounding title, and so on) is a very traditional Gothic Romance film. That means that it has big, creepy castles, a headstrong young woman, a mysterious older man, hints of horror and the supernatural, and romance elements in the contemporary sense of that genre term. It also means that it is very deliberately paced, and that the film will work best for horror mavens who are big fans of understatement. If you love films like Robert Wise\\'s The Haunting (1963), but you also have a taste for late 1960s/early 1970s Spanish and Italian horror, you may love Blood Castle, as well.<br /><br />Baron Janos Dalmar (Carlos Quiney) lives in a large castle on the outskirts of a traditional, unspecified European village. The locals fear him because legend has it that whenever he beds a woman, she soon after ends up dead--the consensus is that he sets his ferocious dogs on them. This is quite a problem because the Baron has a very healthy appetite for women. At the beginning of the film, yet another woman has turned up dead and mutilated.<br /><br />Meanwhile, Dr. Ivanna Rakowsky (Erna Sch\\xc3\\xbcrer) has appeared in the center of the village, asking to be taken to Baron Dalmar\\'s castle. She\\'s an out-of-towner who has been hired by the Baron for her expertise in chemistry. Of course, no one wants to go near the castle. Finally, Ivanna finds a shady individual (who becomes even shadier) to take her. Once there, an odd woman who lives in the castle, Olga (Cristiana Galloni), rejects Ivanna and says that she shouldn\\'t be there since she\\'s a woman. Baron Dalmar vacillates over whether she should stay. She ends up staying, but somewhat reluctantly. The Baron has hired her to try to reverse the effects of severe burns, which the Baron\\'s brother, Igor, is suffering from.<br /><br />Unfortunately, the Baron\\'s brother appears to be just a lump of decomposing flesh in a vat of bizarre, blackish liquid. And furthermore, Ivanna is having bizarre, hallucinatory dreams. Just what is going on at the castle? Is the Baron responsible for the crimes? Is he insane? <br /><br />I wanted to like Blood Castle more than I did. As I mentioned, the film is very deliberate in its pacing, and most of it is very understated. I can go either way on material like that. I don\\'t care for The Haunting (yes, I\\'m in a very small minority there), but I\\'m a big fan of 1960s and 1970s European horror. One of my favorite directors is Mario Bava. I also love Dario Argento\\'s work from that period. But occasionally, Blood Castle moved a bit too slow for me at times. There are large chunks that amount to scenes of not very exciting talking alternated with scenes of Ivanna slowly walking the corridors of the castle.<br /><br />But the atmosphere of the film is decent. Director Jos\\xc3\\xa9 Luis Merino managed more than passable sets and locations, and they\\'re shot fairly well by Emanuele Di Cola. However, Blood Castle feels relatively low budget, and this is a Roger Corman-produced film, after all (which usually means a low-budget, though often surprisingly high quality \"quickie\"). So while there is a hint of the lushness of Bava\\'s colors and complex set decoration, everything is much more minimalist. Of course, it doesn\\'t help that the Retromedia print I watched looks like a 30-year old photograph that\\'s been left out in the sun too long. It appears \"washed out\", with compromised contrast.<br /><br />Still, Merino and Di Cola occasionally set up fantastic visuals. For example, a scene of Ivanna walking in a darkened hallway that\\'s shot from an exaggerated angle, and where an important plot element is revealed through shadows on a wall only. There are also a couple Ingmar Bergmanesque shots, where actors are exquisitely blocked to imply complex relationships, besides just being visually attractive and pulling your eye deep into the frame.<br /><br />The performances are fairly good, and the women--especially Sch\\xc3\\xbcrer--are very attractive. Merino exploits this fact by incorporating a decent amount of nudity. Sch\\xc3\\xbcrer went on to do a number of films that were as much soft corn porn as they were other genres, with English titles such as Sex Life in a Woman\\'s Prison (1974), Naked and Lustful (1974), Strip Nude for Your Killer (1975) and Erotic Exploits of a Sexy Seducer (1977). Blood Castle is much tamer, but in addition to the nudity, there are still mild scenes suggesting rape and bondage, and of course the scenes mixing sex and death.<br /><br />The primary attraction here, though, is probably the story, which is much a slow-burning romance as anything else. The horror elements, the mystery elements, and a somewhat unexpected twist near the end are bonuses, but in the end, Blood Castle is a love story, about a couple overcoming various difficulties and antagonisms (often with physical threats or harms) to be together.'\n",
            "1\n",
            "b\"I was talked into watching this movie by a friend who blubbered on about what a cute story this was.<br /><br />Yuck.<br /><br />I want my two hours back, as I could have done SO many more productive things with my time...like, for instance, twiddling my thumbs. I see nothing redeeming about this film at all, save for the eye-candy aspect of it...<br /><br />3/10 (and that's being generous)\"\n",
            "0\n",
            "b\"Michelle Rodriguez is the defining actress who could be the charging force for other actresses to look out for. She has the audacity to place herself in a rarely seen tough-girl role very early in her career (and pull it off), which is a feat that should be recognized. Although her later films pigeonhole her to that same role, this film was made for her ruggedness.<br /><br />Her character is a romanticized student/fighter/lover, struggling to overcome her disenchanted existence in the projects, which is a little overdone in film...but not by a girl. That aspect of this film isn't very original, but the story goes in depth when the heated relationships that this girl has to deal with come to a boil and her primal rage takes over.<br /><br />I haven't seen an actress take such an aggressive stance in movie-making yet, and I'm glad that she's getting that original twist out there in Hollywood. This film got a 7 from me because of the average story of ghetto youth, but it has such a great actress portraying a rarely-seen role in a minimal budget movie. Great work.\"\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCVzitMX1ff6"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "In particular, we remove `<br />` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xeLQJP1q1ff6"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-7bwoLY1ff7"
      },
      "source": [
        "## Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXzGbm6a1ff7"
      },
      "source": [
        "```python\n",
        "text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```\n",
        "\n",
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training on GPU.\n",
        "So if you're training the model on GPU, you probably want to go with this option to get\n",
        " the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input, like in the code snippet for option 1 above. This can be done after\n",
        " training. We do this in the last section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C_atl8i61ff7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgcPJcUk1ff7"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We choose a simple 1D convnet starting with an `Embedding` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aquí agregué algunas métricas mas para la evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1Meli_fB1ff7"
      },
      "outputs": [],
      "source": [
        "# A integer input for vocab indices.\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Precision\", \"Recall\", \"AUC\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLUtoDj1ff7"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GuQqHaaj1ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 122ms/step - AUC: 0.6566 - Precision: 0.6023 - Recall: 0.6847 - accuracy: 0.6073 - loss: 0.6135 - val_AUC: 0.9463 - val_Precision: 0.8143 - val_Recall: 0.9295 - val_accuracy: 0.8574 - val_loss: 0.3211\n",
            "Epoch 2/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 118ms/step - AUC: 0.9534 - Precision: 0.8847 - Recall: 0.8934 - accuracy: 0.8877 - loss: 0.2741 - val_AUC: 0.9494 - val_Precision: 0.8735 - val_Recall: 0.8891 - val_accuracy: 0.8790 - val_loss: 0.3045\n",
            "Epoch 3/3\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 121ms/step - AUC: 0.9847 - Precision: 0.9447 - Recall: 0.9397 - accuracy: 0.9420 - loss: 0.1525 - val_AUC: 0.9454 - val_Precision: 0.8389 - val_Recall: 0.9219 - val_accuracy: 0.8712 - val_loss: 0.3585\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1cf9b935810>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 3\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHyJUEB71ff8"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5vmIZXhh1ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - AUC: 0.9351 - Precision: 0.8358 - Recall: 0.8954 - accuracy: 0.8598 - loss: 0.3812\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.3806702792644501,\n",
              " 0.8611199855804443,\n",
              " 0.8359130620956421,\n",
              " 0.898639976978302,\n",
              " 0.9358351826667786]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpER8OXp1ff8"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model (using the weights we just trained):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uTVWplxw1ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - AUC: 0.9354 - Precision: 0.8361 - Recall: 0.8954 - accuracy: 0.8600 - loss: 0.3808\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.3806701600551605,\n",
              " 0.8611199855804443,\n",
              " 0.8359130620956421,\n",
              " 0.898639976978302,\n",
              " 0.9358351826667786]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# A string input\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Precision\", \"Recall\", \"AUC\"] \n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_from_scratch",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
